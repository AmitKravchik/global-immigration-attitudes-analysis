# Data Analysis and Visualization of GDELT Tone Chart Data

This repository provides a comprehensive solution for analyzing media tone using data from the [GDELT Project](https://www.gdeltproject.org/). At its core, the project consists of two major components:

1. **Interactive Data Analysis Notebook:**  
   A Jupyter Notebook that demonstrates how to load, process, and visualize the aggregated tone chart data with interactive Plotly charts. This notebook is designed to help you explore trends in media tone and article counts over time and create publication-ready visualizations with customizable options.

2. **Robust Data Pipeline:**  
   A high-performance Python pipeline that fetches, processes, and aggregates tone chart data from GDELT. The pipeline leverages multi-threading and multi-processing for efficient data retrieval and parsing, saving the processed results to disk for further analysis.

---

## Data Analysis Notebook

The [data_analysis.ipynb](notebooks/data_analysis.ipynb) notebook is the centerpiece for exploring the GDELT tone data. Key features include:

- **Interactive Visualizations:**  
  The notebook uses Plotly Express and Plotly Graph Objects to create dynamic charts such as choropleth maps, scatter plots, and time-series visualizations. These visualizations enable you to investigate media tone trends by country, month, or year.

- **Customization Options:**  
  Easily adjust figure sizes, disable hover features, and fine-tune annotations to produce clear and publication-ready graphics. For example, you can reduce hover information for a cleaner look or adjust margins to emphasize the map area.

- **Step-by-Step Explanations:**  
  Detailed markdown cells explain each part of the analysis process—from data loading and aggregation to the creation of visualizations—making the notebook an excellent resource for both beginners and advanced users.

- **Exploratory Data Analysis:**  
  The notebook demonstrates how to slice and dice the aggregated JSON outputs (generated by the pipeline) to uncover insights about media coverage and tone changes over time.

To get started with the notebook, run:

```bash
jupyter notebook notebooks/data_analysis.ipynb
```

---

## Data Pipeline

The data pipeline is responsible for obtaining and processing the raw tone chart data from GDELT. It is structured to handle large volumes of data efficiently and includes the following stages:

- **Date Interval Generation:**  
  The pipeline divides the overall time period into weekly intervals, ensuring comprehensive coverage of the data.

- **Data Fetching (Stage 1):**  
  For each country and date interval, the pipeline sends a query to the GDELT API to retrieve tone chart data. This step leverages concurrent processing using `ThreadPoolExecutor` and `ProcessPoolExecutor`.

- **HTML Fetching (Stage 2):**  
  A dedicated thread pool downloads HTML content for each article linked in the tone chart data, ensuring quick retrieval.

- **HTML Parsing (Stage 3):**  
  The pipeline parses each downloaded HTML document (extracting both body and title) using CPU-bound processes, which optimizes performance by parallelizing intensive parsing tasks.

- **Data Saving (Stage 4):**  
  Finally, the processed tone chart data is saved to disk as JSON files. These files are organized in a structured directory hierarchy based on country and year, making them easy to access for further analysis in the notebook.

- **Logging and Error Handling:**  
  Throughout the process, detailed logging (using [Loguru](https://github.com/Delgan/loguru)) tracks the progress and any errors encountered, which is vital for debugging and ensuring data integrity.

To run the pipeline for a specific source country, use the following command:

```bash
python main.py US
```

Replace `US` with the desired country code. The results are saved under `output2/<country>/<year>/<month>.json`.

---

## Repository Structure

```
├── src/
│   ├── gdelt/
│   │   ├── client.py            # GDELT API client for handling requests
│   │   ├── article.py           # Models for Article, ToneChart, etc.
│   │   ├── responses.py         # Response models for tone chart data
│   │   └── query_params.py      # Definitions for constructing GDELT queries
│   ├── parsers/
│   │   ├── body_parser.py       # Parser for extracting article body text
│   │   └── title_parser.py      # Parser for extracting article title
│   ├── settings.py              # Configuration and settings for the pipeline
│   └── utils/
│       └── requests.py          # Utility functions for HTTP requests
├── notebooks/
│   └── data_analysis.ipynb      # Jupyter Notebook for exploring and visualizing data
├── output2/                     # Output directory for generated JSON results
└── main.py                      # Main entry point for running the pipeline
```

---

## Installation

1. **Clone the Repository:**

   ```bash
   git clone https://github.com/AmitKrvchik/global-immigration-attitudes-analysis.git
   cd global-immigration-attitudes-analysis
   ```

2. **Create and Activate a Virtual Environment:**

   ```bash
   python3 -m venv venv
   source venv/bin/activate   # On Windows: venv\Scripts\activate
   ```

3. **Install Dependencies:**

   Make sure you have Python 3.7+ installed, then run:

   ```bash
   pip install -r requirements.txt
   ```

---

## Configuration

The project uses a `Settings` class (in `src/settings.py`) to manage configuration parameters such as:

- **Date Range:**  
  The start and end dates for data retrieval.
- **Countries:**  
  The list of countries to process.
- **API Threads:**  
  The number of threads to use for concurrent API calls.
- **Query & Theme:**  
  The base query and theme used for GDELT requests.

Modify these settings directly in the file or via environment variables as needed.

---

## Contributing

Contributions are welcome! Please fork the repository and submit a pull request with your improvements.

---

## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.

---

This README emphasizes the interactive data analysis and visualization aspect while providing detailed information about the robust pipeline that powers the project. Adjust sections as necessary to better match your project’s specifics and future enhancements.
